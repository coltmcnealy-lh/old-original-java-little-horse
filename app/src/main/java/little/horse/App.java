/*
 * This Java source file was generated by the Gradle 'init' task.
 */
package little.horse;

import java.lang.reflect.Field;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.regex.Pattern;

import com.jayway.jsonpath.JsonPath;

import org.apache.kafka.clients.admin.NewTopic;
import org.apache.kafka.streams.KafkaStreams;
import org.apache.kafka.streams.Topology;

import little.horse.api.LittleHorseAPI;
import little.horse.api.runtime.WFRunTopology;
import little.horse.api.util.APIStreamsContext;
import little.horse.common.Config;
import little.horse.common.objects.BaseSchema;
import little.horse.common.objects.DigestIgnore;
import little.horse.common.objects.metadata.EdgeCondition;
import little.horse.common.objects.metadata.Edge;
import little.horse.common.objects.metadata.Node;
import little.horse.common.objects.metadata.VariableAssignment;
import little.horse.common.objects.metadata.WFRunVariableDef;
import little.horse.common.objects.rundata.ThreadRun;
import little.horse.common.objects.rundata.WFRun;
import little.horse.common.util.Constants;
import little.horse.common.util.LHUtil;


class FrontendAPIApp {
    private static void createKafkaTopics(Config config) {
        int partitions = 1;
        short replicationFactor = 1;

        String[] topics = {
            config.getWFSpecActionsTopic(),
            config.getWFSpecTopic(),
            config.getWFSpecIntermediateTopic(),
            config.getTaskDefNameKeyedTopic(),
            config.getTaskDefTopic(),
            config.getWFSpecNameKeyedTopic(),
            config.getExternalEventDefNameKeyedTopic(),
            config.getWFRunTopic(),
            config.getExternalEventDefTopic()
        };
        for (String topicName : topics) {
            NewTopic newTopic = new NewTopic(topicName, partitions, replicationFactor);
            config.createKafkaTopic(newTopic);
        }
    }

    /**
     * Does three things:
     * 1. Sets up a KafkaStreams topology for processing WFSpec, TaskDef, and WFRun updates.
     * 2. Sets up a LittleHorseAPI to respond to metadata control requests.
     * 3. Sets up a listener for new WFSpecs that deploys them to kubernetes (if necessary).
     */
    public static void run() {
        Config config = null;
        config = new Config();

        FrontendAPIApp.createKafkaTopics(config);
        Topology topology = new Topology();

        // TaskDefTopology.addStuff(topology, config);
        // WFSpecTopology.addStuff(topology, config);
        // ExternalEventDefTopology.addStuff(topology, config);

        WFRunTopology.addStuff(
            topology,
            config,
            Pattern.compile(config.getWFRunTopic())
        );

        KafkaStreams streams = new KafkaStreams(topology, config.getStreamsConfig());

        APIStreamsContext context = new APIStreamsContext(streams);

        LittleHorseAPI lapi = new LittleHorseAPI(config, context);

        Runtime.getRuntime().addShutdownHook(new Thread(config::cleanup));
        Runtime.getRuntime().addShutdownHook(new Thread(lapi::cleanup));
        Runtime.getRuntime().addShutdownHook(new Thread(streams::close));

        streams.start();
        lapi.run();
    }
}


class Thing {
    // @IncludeInDigest
    public String mystring;
    @DigestIgnore
    public Object foobar;

    @DigestIgnore
    public int myint = 123;

    public void printDigest() throws IllegalAccessException {
        for (Field field: this.getClass().getDeclaredFields()) {
            field.setAccessible(true);
            if (field.isAnnotationPresent(DigestIgnore.class)) {
                System.out.println(field.get(this).toString());
            }
        }
    }
}

public class App {
    public static void main(String[] args) {
        if (args.length > 0 && args[0].equals("api")) {
            System.out.println("running the app");
            FrontendAPIApp.run();
        } else {
            Node node = new Node();
            node.name = "asdf";
            node.outgoingEdges = new ArrayList<>();
            node.outgoingEdges.add(new Edge());

            node.variables = new HashMap<>();
            VariableAssignment assn = new VariableAssignment();
            assn.defaultValue = "default";
            node.variables.put("foobar", assn);

            node.variables.put("notfoobar", assn);
            System.out.println(node.getId());

            System.out.println(node.getId());

            // String json = "{\"foo\": 1234, \"name\": \"task1\", \"guid\": \"06ab9216-a34c-4845-b594-4b1a90e8d3ee\", \"dockerImage\": \"little-horse-daemon\", \"bashCommand\": [\"python3\", \"/examples/task1.py\", \"<<personName>>\"], \"stdin\": null}";
            // Object obj = JsonPath.parse(json).read("$.foo");
            // System.out.println(obj.getClass());

            // ArrayList<Object> thing = new ArrayList<Object>();
            // NodeSchema node = new NodeSchema();
            // // node.guid = "asdf;";
            // thing.add(node);
            // System.out.println(thing.toString());

            // System.out.println("\n\n\n\n\n\n\n");

            // String data = "{\"threadRuns\":[{\"id\":0}]}";
            // WFRunSchema schema = BaseSchema.fromString(data, WFRunSchema.class);
            // ThreadRunSchema tr = schema.threadRuns.get(0);

            // LHUtil.log(tr.threadSpec);
            
        }
    }
}
