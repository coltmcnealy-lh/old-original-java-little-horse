/*
 * This Java source file was generated by the Gradle 'init' task.
 */
package little.horse;

import java.util.Collections;
import java.util.Properties;
import java.util.regex.Pattern;

import com.fasterxml.jackson.databind.ObjectMapper;
import com.jayway.jsonpath.DocumentContext;
import com.jayway.jsonpath.InvalidJsonException;
import com.jayway.jsonpath.JsonPath;

import org.apache.commons.lang3.compare.ObjectToStringComparator;
import org.apache.kafka.clients.admin.NewTopic;
import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.apache.kafka.streams.KafkaStreams;
import org.apache.kafka.streams.Topology;

import little.horse.api.APIStreamsContext;
import little.horse.api.TaskDefTopology;
import little.horse.api.WFSpecDeployer;
import little.horse.api.WFSpecTopology;
import little.horse.lib.Config;
import little.horse.lib.Constants;
import little.horse.lib.NodeSchema;
import little.horse.lib.NullWFEventActor;
import little.horse.lib.TaskDaemonEventActor;
import little.horse.lib.TaskDef;
import little.horse.lib.WFEventProcessorActor;
import little.horse.lib.WFRunTopology;
import little.horse.lib.WFSpec;
import little.horse.lib.WFSpecSchema;
import little.horse.lib.kafkaStreamsSerdes.WFSpecDeSerializer;


class FrontendAPIApp {
    private static void createKafkaTopics(Config config) {
        int partitions = 1;
        short replicationFactor = 1;

        String[] topics = {
            config.getWFSpecActionsTopic(),
            config.getWFSpecTopic(),
            config.getWFSpecIntermediateTopic(),
            config.getTaskDefNameKeyedTopic(),
            config.getTaskDefTopic(),
            config.getWFSpecNameKeyedTopic()
        };
        for (String topicName : topics) {
            NewTopic newTopic = new NewTopic(topicName, partitions, replicationFactor);
            config.createKafkaTopic(newTopic);
        }
    }

    /**
     * Does three things:
     * 1. Sets up a KafkaStreams topology for processing WFSpec, TaskDef, and WFRun updates.
     * 2. Sets up a LittleHorseAPI to respond to metadata control requests.
     * 3. Sets up a listener for new WFSpecs that deploys them to kubernetes (if necessary).
     */
    public static void run() {
        Config config = null;
        config = new Config();

        FrontendAPIApp.createKafkaTopics(config);
        Topology topology = new Topology();

        TaskDefTopology.addStuff(topology, config);
        WFSpecTopology.addStuff(topology, config);

        WFEventProcessorActor actor = new NullWFEventActor();
        WFRunTopology.addStuff(
            topology,
            config,
            config.getAllWFRunTopicsPattern(),
            actor
        );

        KafkaStreams streams = new KafkaStreams(topology, config.getStreamsConfig());

        APIStreamsContext context = new APIStreamsContext(streams);
        context.setWFSpecNameStoreName(Constants.WF_SPEC_NAME_STORE);
        context.setWFSpecGuidStoreName(Constants.WF_SPEC_GUID_STORE);
        context.setTaskDefGuidStoreName(Constants.TASK_DEF_GUID_STORE);
        context.setTaskDefNameStoreName(Constants.TASK_DEF_NAME_STORE);
        context.setWFRunStoreName(Constants.WF_RUN_STORE);

        LittleHorseAPI lapi = new LittleHorseAPI(config, context);

        Properties props = config.getConsumerConfig("wfSpecDeployer");
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, WFSpecDeSerializer.class.getName());

        KafkaConsumer<String, WFSpecSchema> consumer = new KafkaConsumer<>(
            props
        );
        consumer.subscribe(Collections.singletonList(config.getWFSpecActionsTopic()));
        WFSpecDeployer deployer = new WFSpecDeployer(consumer, config);
        Thread deployerThread = new Thread(() -> deployer.run());

        Runtime.getRuntime().addShutdownHook(new Thread(deployer::shutdown));
        Runtime.getRuntime().addShutdownHook(new Thread(config::cleanup));
        Runtime.getRuntime().addShutdownHook(new Thread(lapi::cleanup));
        Runtime.getRuntime().addShutdownHook(new Thread(streams::close));

        deployerThread.start();
        streams.start();
        lapi.run();
    }
}


class DaemonApp {
    public static void run() throws Exception {
        Config config = new Config();

        // just need to set up the topology and run it.

        WFSpec wfSpec = WFSpec.fromIdentifier(config.getWfSpecGuid(), config);

        NodeSchema node = wfSpec.getModel().nodes.get(config.getNodeName());
        TaskDef td = TaskDef.fromIdentifier(node.taskDefinitionName, config);

        WFEventProcessorActor actor = new TaskDaemonEventActor(
            wfSpec,
            node,
            td,
            config
        );

        Pattern pattern = Pattern.compile(wfSpec.getModel().kafkaTopic);
        Topology topology = new Topology();

        WFRunTopology.addStuff(topology, config, pattern, actor);
        KafkaStreams streams = new KafkaStreams(
            topology,
            config.getStreamsConfig(config.getNodeName())
        );
        Runtime.getRuntime().addShutdownHook(new Thread(streams::close));
        streams.start();
    }
}

class Thing {
    public String mystring;
    public Object foobar;
}

public class App {
    public static void main(String[] args) {
        if (args.length > 0 && args[0].equals("daemon")) {
            try {
                DaemonApp.run();
            } catch(Exception exn) {
                exn.printStackTrace();
            }
        } else if (args.length > 0 && args[0].equals("api")) {
            FrontendAPIApp.run();
        } else {
            String json = "{\"foo\": 1234, \"b{}\"ar\": {\"asdf\": 1234}}";

            try {
                Object out = new ObjectMapper().readValue(json, Object.class);
                System.out.println(out);
                Thing thing = new Thing();
                thing.mystring = "hellothere";
                thing.foobar = out;

                System.out.println(new ObjectMapper().writeValueAsString(thing));
            } catch (Exception exn) {
                exn.printStackTrace();
            }
            // DocumentContext jdoc;
            // try {
            //     jdoc = JsonPath.parse(json);
            // } catch(InvalidJsonException exn) {
            //     exn.printStackTrace();
            //     return;
            // }
            // Object result = jdoc.read("$");
            // System.out.println(result);
            
            // try {
            //     System.out.println(new ObjectMapper().writeValueAsString(result));
            // } catch(Exception exn) {
            //     exn.printStackTrace();
            // }
        }
    }
}
