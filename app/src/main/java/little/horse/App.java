/*
 * This Java source file was generated by the Gradle 'init' task.
 */
package little.horse;

import java.util.Arrays;

import org.apache.kafka.clients.admin.NewTopic;
import org.apache.kafka.streams.KafkaStreams;
import org.apache.kafka.streams.Topology;

import little.horse.api.LittleHorseAPI;
import little.horse.api.metadata.MetadataTopologyBuilder;
import little.horse.common.Config;
import little.horse.common.exceptions.LHConnectionError;
import little.horse.common.objects.metadata.CoreMetadata;
import little.horse.common.objects.metadata.ExternalEventDef;
import little.horse.common.objects.metadata.TaskDef;
import little.horse.common.objects.metadata.WFSpec;
import little.horse.common.objects.rundata.WFRun;
import little.horse.common.util.LHUtil;
import little.horse.lib.deployers.examples.docker.DDConfig;
import little.horse.lib.deployers.examples.docker.DockerTaskWorker;
import little.horse.lib.deployers.examples.docker.DockerWorkflowWorker;

class FrontendAPIApp {
    private static void createKafkaTopics(Config config) {
        int partitions = config.getDefaultPartitions();
        short replicationFactor = (short) config.getDefaultReplicas();

        for (Class<? extends CoreMetadata> cls: Arrays.asList(
            WFSpec.class, TaskDef.class, ExternalEventDef.class, WFRun.class
        )) {
            LHUtil.log("About to create topics for ", cls.getName());
            config.createKafkaTopic(
                new NewTopic(
                    CoreMetadata.getIdKafkaTopic(config, cls),
                    partitions,
                    replicationFactor
                )
            );

            config.createKafkaTopic(
                new NewTopic(
                    CoreMetadata.getAliasKafkaTopic(config, cls),
                    partitions,
                    replicationFactor
                )
            );
        }
    }

    /**
     * Does three things:
     * 1. Sets up a KafkaStreams topology for processing WFSpec, TaskDef, and WFRun updates.
     * 2. Sets up a LittleHorseAPI to respond to metadata control requests.
     * 3. Sets up a listener for new WFSpecs that deploys them to kubernetes (if necessary).
     */
    public static void run() {
        Config config = null;
        config = new Config();

        LHUtil.log("Creating kafka topics");
        FrontendAPIApp.createKafkaTopics(config);
        Topology topology = new Topology();

        for (Class<? extends CoreMetadata> cls: Arrays.asList(
            WFSpec.class, TaskDef.class, ExternalEventDef.class, WFRun.class
        )) {
            MetadataTopologyBuilder.addStuff(topology, config, cls);
        }

        KafkaStreams streams = new KafkaStreams(topology, config.getStreamsConfig(
            "central-api"
        ));
        LittleHorseAPI lapi = new LittleHorseAPI(config, streams);

        Runtime.getRuntime().addShutdownHook(new Thread(config::cleanup));
        Runtime.getRuntime().addShutdownHook(new Thread(lapi::cleanup));
        Runtime.getRuntime().addShutdownHook(new Thread(() -> {
            System.out.println("Closing streams!");
            streams.close();
        }));

        streams.start();
        lapi.run();
    }
}

public class App {
    public static void main(String[] args) throws LHConnectionError {
        if (args.length > 0 && args[0].equals("api")) {
            System.out.println("running the app");
            FrontendAPIApp.run();
        } else if (args.length > 0 && args[0].equals("docker-workflow-worker")) {
            new DockerWorkflowWorker(new DDConfig(), new Config()).run();

        } else if (args.length > 0  && args[0].equals("docker-task-worker")) {
            try {
                Thread.sleep(3000);
            } catch(InterruptedException exn) {}

            new DockerTaskWorker(new DDConfig(), new Config()).run();

        } else {
            System.out.println("TODO: run some experiment for funsies.");
        }

    }
}
